# potensia_ai/ai_tools/writer/generator.py
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import asyncio
import datetime
import random
import traceback
from openai import OpenAI
from anthropic import Anthropic
from core.config import settings
from ai_tools.writer.prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
from ai_tools.writer.topic_refiner import refine_topic

openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
claude_client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)

MAX_RETRY = 2               # ëª¨ë¸ë³„ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
BACKOFF_MIN = 1             # ìµœì†Œ ëŒ€ê¸° (ì´ˆ)
BACKOFF_MAX = 2             # ìµœëŒ€ ëŒ€ê¸° (ì´ˆ)


def log_event(model: str, topic: str, status: str, error: str | None = None):
    """ê¸°ë³¸ ì½˜ì†” ë¡œê·¸ (ìš´ì˜ ì‹œ Loguru/Sentryë¡œ êµì²´ ê°€ëŠ¥)"""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{model}] [{status}] Topic: {topic}")
    if error:
        print(f" â””â”€ Error: {error}\n")


async def try_model(model_name: str, topic: str, user_prompt: str) -> str | None:
    """GPT or Claude ì‹¤í–‰ (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
    try:
        if model_name == "GPT":
            resp = openai_client.chat.completions.create(
                model=settings.MODEL_PRIMARY,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                # Note: reasoning models don't support temperature
                max_completion_tokens=10000,  # Very high limit for reasoning models
            )
            text = resp.choices[0].message.content

            # Debug: Check if we got empty content due to reasoning token usage
            if not text or not text.strip():
                usage = resp.usage
                log_event(model_name, topic, "EMPTY_CONTENT",
                         f"Reasoning tokens: {usage.completion_tokens_details.reasoning_tokens if hasattr(usage.completion_tokens_details, 'reasoning_tokens') else 'N/A'}, "
                         f"Total completion: {usage.completion_tokens}")
                return None

            return text.strip()

        elif model_name == "Claude":
            resp = claude_client.messages.create(
                model=settings.MODEL_FALLBACK,
                max_tokens=5000,
                temperature=0.7,
                system=SYSTEM_PROMPT,
                messages=[{"role": "user", "content": user_prompt}],
            )
            text = resp.content[0].text.strip()
            return text if text else None

    except Exception as e:
        log_event(model_name, topic, "FAIL", f"{type(e).__name__}: {e}")
        traceback.print_exc()
        return None


async def generate_content(topic: str) -> str:
    """
    Step 1ï¸âƒ£: Topic Refinerë¡œ ì§ˆë¬¸í˜• ì œëª© ìƒì„±
    Step 2ï¸âƒ£: ìƒì„±ëœ ì œëª©(generated_topic)ì„ user_promptì— ì‚½ì…
    Step 3ï¸âƒ£: GPT-4o â†’ Claude â†’ GPT-4o ì¬ì‹œë„ â†’ Claude ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ ë£¨í”„)
    Step 4ï¸âƒ£: ëª¨ë“  ì‹¤íŒ¨ ì‹œ RuntimeError (FastAPI 500)
    """

    # 1ï¸âƒ£ topic refinement (ì§ˆë¬¸í˜• ì œëª©ìœ¼ë¡œ ë³´ì •)
    try:
        generated_topic = await refine_topic(topic)
        print(f"[Refined Topic] {topic} â†’ {generated_topic}")
    except Exception as e:
        generated_topic = topic
        print(f"[TopicRefiner Error] {e} (fallback to original topic)")

    # 2ï¸âƒ£ ë³´ì •ëœ topicì„ user_promptì— ì‚½ì…
    user_prompt = USER_PROMPT_TEMPLATE.format(topic=generated_topic)

    # 3ï¸âƒ£ ëª¨ë¸ ì‹¤í–‰ ìˆœì„œ ì •ì˜ (GPT ìš°ì„ , ClaudeëŠ” ìµœì¢… fallbackë§Œ)
    model_sequence = ["GPT", "GPT", "GPT", "Claude"]

    # 4ï¸âƒ£ ì‹¤í–‰ ë£¨í”„
    for attempt, model in enumerate(model_sequence, start=1):
        retry_round = (attempt - 1) // 2 + 1
        retry_label = "RETRY_START" if attempt > 2 else "START"
        log_event(model, generated_topic, retry_label)

        content = await try_model(model, generated_topic, user_prompt)

        if content:
            # ì„±ê³µ ë¡œê·¸
            success_label = "RETRY_SUCCESS" if attempt > 2 else "SUCCESS"
            log_event(model, generated_topic, success_label)
            return content

        # ì‹¤íŒ¨ ì‹œ ë¡œê·¸
        fail_label = "RETRY_FAIL" if attempt > 2 else "FAIL"
        log_event(model, generated_topic, fail_label)

        # ìë™ ë°±ì˜¤í”„ (ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ ê²½ìš°ë§Œ)
        if attempt < len(model_sequence):
            backoff = random.uniform(BACKOFF_MIN, BACKOFF_MAX)
            print(f"â³ Waiting {backoff:.1f}s before next retry...\n")
            await asyncio.sleep(backoff)

        # ì¬ì‹œë„ ì œí•œ
        if retry_round > MAX_RETRY:
            log_event(model, generated_topic, "RETRY_LIMIT_REACHED")
            break

    # 5ï¸âƒ£ ëª¨ë“  ëª¨ë¸/ì¬ì‹œë„ ì‹¤íŒ¨
    log_event("SYSTEM", generated_topic, "TOTAL_FAIL", "All GPT & Claude attempts failed")
    raise RuntimeError(f"All model attempts failed for topic: {generated_topic}")


# ============================================================
# ğŸ”¹ ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
# ============================================================
if __name__ == "__main__":
    async def test():
        test_topic = "ìƒì• ìµœì´ˆ ì£¼íƒë‹´ë³´ëŒ€ì¶œ, ì‹ ì²­ ìê²©ê³¼ í•œë„Â·ê¸ˆë¦¬ëŠ”?"
        print(f"\n{'='*60}")
        print(f"Testing Content Generator")
        print(f"{'='*60}\n")
        print(f"Input Topic: {test_topic}\n")

        try:
            result = await generate_content(test_topic)
            print(f"\n{'='*60}")
            print(f"Generated Content:")
            print(f"{'='*60}\n")
            print(result)
        except Exception as e:
            print(f"\n[ERROR] Content generation failed: {e}")

    asyncio.run(test())
